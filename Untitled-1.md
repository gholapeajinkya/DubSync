https://github.com/suno-ai/bark?tab=readme-ov-file

https://huggingface.co/spaces/suno/bark


1.30 => 40

source venv/bin/activate
deactivate



https://docs.streamlit.io/develop/api-reference/layout


streamlit run main_ui.py

âœ… Best Practice Pipeline
ðŸ”Š Split original audio into timestamped segments (e.g., Whisper)

ðŸŒ Translate each segment

âœï¸ Optionally rephrase to fit time better

ðŸ—£ï¸ Use high-quality TTS (SSML or segment-based)

ðŸ”‡ Add silence to fill timing gaps

ðŸ” Merge all into final track aligned with original video


Diarization, in the context of speech recognition and audio processing, refers to the process of identifying and separating different speakers in an audio recording. It's essentially about determining "who said what" within an audio file, typically without knowing the speakers' identities in advance. This process segments the audio into distinct sections, each attributed to a different speaker. 

# New virtual env: 

## How to create - `python3.10 -m venv venv310`
## Activate - `source venv310/bin/activate`
## Freeze requiremets - `pip freeze > requirements310.txt`

